"""
ä¸æœ¬åœ°ç‰ˆæœ¬å®Œå…¨ä¸€è‡´çš„é“¾æ¥æŸ¥æ‰¾å’Œè¿‡æ»¤é€»è¾‘ï¼Œä¸“æ³¨äºPDFä¸‹è½½å’Œé“¾æ¥è¾“å‡ºï¼Œå®Œæ•´å¤„ç†èƒ½åŠ›
"""

from __future__ import annotations
import os, time
from pathlib import Path
from urllib.parse import urlparse
import urllib3
import tldextract
from dotenv import load_dotenv

from .search_google import build_query, search_links
from .ai_filter import is_link_relevant
from .link_crawler import bfs
from .pdf_downloader import download_pdf_if_available

urllib3.disable_warnings()


def run_analysis_for_city(city: str) -> dict:
    """ä¸æœ¬åœ°ç‰ˆæœ¬å®Œå…¨ä¸€è‡´çš„é“¾æ¥æŸ¥æ‰¾å’Œè¿‡æ»¤"""
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    SERPER_API_KEY = os.getenv("SERPER_API_KEY")
    
    if not OPENAI_API_KEY or not SERPER_API_KEY:
        raise RuntimeError("OPENAI_API_KEY ã¨ SERPER_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

    # ä¸æœ¬åœ°ç‰ˆæœ¬å®Œå…¨ä¸€è‡´çš„å…³é”®è¯
    keywords = [
        "éƒ½å¸‚è¨ˆç”»å›³",
        "ç”¨é€”åœ°åŸŸ", 
        "å»ºè”½ç‡",
        "å®¹ç©ç‡",
        "é–‹ç™ºæŒ‡å°è¦ç¶±",
        "å»ºç¯‰åŸºæº–æ³•"
    ]
    
    query = build_query(city, keywords)
    print(f"ğŸ” Initial Search Query: {query}")

    # ä¸æœ¬åœ°ç‰ˆæœ¬ä¸€è‡´çš„æœç´¢å‚æ•°
    seed_links = search_links(query, SERPER_API_KEY, num_results=10)
    print(f"ğŸŒ± Found {len(seed_links)} seed links.")
    
    if not seed_links:
        return {"error": "ã‚·ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"}

    # ä¸æœ¬åœ°ç‰ˆæœ¬å®Œå…¨ä¸€è‡´çš„çˆ¬è™«å‚æ•°
    crawled_links = bfs(seed_links, seed_links[0], max_depth=2, max_total=120)
    print(f"ğŸ”— Crawled to {len(crawled_links)} total unique links (including seeds).")

    pdf_dir = Path("downloaded_pdfs")
    pdf_dir.mkdir(exist_ok=True)

    relevant_links = []
    pdf_downloads = []
    processed_count = 0
    
    # ä¿®å¤åŸŸåè¿‡æ»¤é—®é¢˜ï¼šä½¿ç”¨ä¸æœ¬åœ°ç‰ˆæœ¬ä¸€è‡´çš„æ–¹å¼æå–åŸŸå
    base_domain = tldextract.extract(seed_links[0]).registered_domain if seed_links else ""
    print(f"ğŸ  Base domain for filtering: {base_domain}")
    
    # ä¸æœ¬åœ°ç‰ˆæœ¬ä¸€è‡´çš„å¤„ç†æ•°é‡
    max_process = min(30, len(crawled_links))  # æ¢å¤åˆ°30ä¸ª
    
    # ä¸æœ¬åœ°ç‰ˆæœ¬ä¸€è‡´çš„å¤„ç†é€»è¾‘
    for i, url in enumerate(crawled_links):
        if processed_count >= max_process:
            print(f"Reached max_links limit of {max_process}. Stopping processing.")
            break
        
        # æ¯5ä¸ªé“¾æ¥è¾“å‡ºä¸€æ¬¡è¿›åº¦
        if i % 5 == 0:
            print(f"ğŸ”„ Progress: {i}/{len(crawled_links)} links checked, {processed_count} relevant found")
        
        try:
            if not is_link_relevant(url, city, base_domain, OPENAI_API_KEY):
                print(f"âŒ [Filter] Skipping irrelevant link: {url}")
                continue
        except Exception as e:
            print(f"âš ï¸ [Filter Error] {url}: {e}")
            continue
        
        print(f"âœ… [Relevant] Processing link ({processed_count + 1}/{max_process}): {url}")
        
        link_info = {
            "url": url,
            "type": "PDF" if url.lower().endswith(".pdf") else "HTML",
            "status": "relevant"
        }
        
        # å°è¯•ä¸‹è½½PDF
        if url.lower().endswith(".pdf"):
            try:
                pdf_path = download_pdf_if_available(url, str(pdf_dir))
                if pdf_path:
                    link_info["downloaded"] = True
                    link_info["local_path"] = f"/files/{Path(pdf_path).name}"
                    pdf_downloads.append({
                        "original_url": url,
                        "local_path": f"/files/{Path(pdf_path).name}",
                        "filename": Path(pdf_path).name
                    })
                else:
                    link_info["downloaded"] = False
            except Exception as e:
                print(f"âš ï¸ PDF download error for {url}: {e}")
                link_info["downloaded"] = False
        else:
            link_info["downloaded"] = False
        
        relevant_links.append(link_info)
        processed_count += 1
        
        # ä¸æœ¬åœ°ç‰ˆæœ¬ä¸€è‡´çš„å»¶è¿Ÿ
        time.sleep(1.5)  # æ¢å¤åˆ°1.5ç§’

    # ç”Ÿæˆç®€åŒ–æŠ¥å‘Š
    report_content = f"# {city} å»ºç¯‰è¦åˆ¶é–¢é€£ãƒªãƒ³ã‚¯èª¿æŸ»çµæœ\n\n"
    report_content += f"## æ¦‚è¦\n"
    report_content += f"- æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}\n"
    report_content += f"- åˆæœŸæ¤œç´¢çµæœ: {len(seed_links)} ä»¶\n"
    report_content += f"- ã‚¯ãƒ­ãƒ¼ãƒ«ç·æ•°: {len(crawled_links)} ä»¶\n"
    report_content += f"- å‡¦ç†å¯¾è±¡: {max_process} ä»¶\n"
    report_content += f"- é–¢é€£æ€§ã®é«˜ã„ãƒªãƒ³ã‚¯: {len(relevant_links)} ä»¶\n"
    report_content += f"- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸPDF: {len(pdf_downloads)} ä»¶\n\n"
    
    if relevant_links:
        report_content += "## é–¢é€£æ€§ã®é«˜ã„ãƒªãƒ³ã‚¯ä¸€è¦§\n\n"
        for i, link in enumerate(relevant_links, 1):
            report_content += f"### {i}. {link['type']} ãƒªãƒ³ã‚¯\n"
            report_content += f"- URL: {link['url']}\n"
            if link['downloaded']:
                report_content += f"- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: æˆåŠŸ ({link['local_path']})\n"
            else:
                report_content += f"- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {'PDFä»¥å¤–' if not link['url'].lower().endswith('.pdf') else 'å¤±æ•—'}\n"
            report_content += "\n"
    else:
        report_content += "## çµæœ\n\n"
        report_content += "ä»Šå›ã®æ¤œç´¢ã§ã¯é–¢é€£æ€§ã®é«˜ã„ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"
        report_content += "- AIãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®åˆ¤å®šãŒå³ã—ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\n"
        report_content += "- åˆ¥ã®éƒ½å¸‚ã§è©¦ã—ã¦ã¿ã¦ãã ã•ã„\n\n"
    
    if pdf_downloads:
        report_content += "## ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿PDFãƒ•ã‚¡ã‚¤ãƒ«\n\n"
        for pdf in pdf_downloads:
            report_content += f"- [{pdf['filename']}]({pdf['local_path']}) (å…ƒURL: {pdf['original_url']})\n"

    # ç¡®ä¿å§‹ç»ˆè¿”å›æœ‰æ•ˆçš„å“åº”ç»“æ„
    return {
        "summary": f"æ¤œç´¢å®Œäº†: {len(relevant_links)}ä»¶ã®é–¢é€£ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹ã€{len(pdf_downloads)}ä»¶ã®PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        "report": report_content,
        "relevant_links": relevant_links,
        "pdf_downloads": pdf_downloads,
        "statistics": {
            "total_crawled": len(crawled_links),
            "processed_count": max_process,
            "relevant_count": len(relevant_links),
            "pdf_count": len(pdf_downloads)
        }
    }
