import os
from dotenv import load_dotenv
from pathlib import Path
import httpx
from urllib.parse import urlparse
import tldextract

# å°å…¥æ‰€æœ‰éœ€è¦çš„æ¨¡çµ„
from .initialize_credentials import initialize_google_credentials
from .search_google import build_query, search_links, get_Google Search_service
from .ai_filter import is_link_relevant
from .link_crawler import bfs
from .pdf_downloader import download_pdf_if_available
from .summarizer import summarize_text_from_url_or_pdf, generate_zone_regulations_txt, generate_sources_txt

def run_analysis_for_city(city: str) -> dict:
    load_dotenv()
    
    # æ­¥é©Ÿ 1: åˆå§‹åŒ– Google æ†‘è­‰ï¼Œé€™æ˜¯æœ€å…ˆè¦åšçš„äº‹
    try:
        initialize_google_credentials()
        print("âœ… Googleã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®èªè¨¼ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
    except Exception as e:
        return {"error": f"Googleèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}"}

    # æ­¥é©Ÿ 2: ç²å–å…¶ä»–å¿…è¦çš„ç’°å¢ƒè®Šæ•¸
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    SEARCH_ENGINE_ID = os.getenv("SEARCH_ENGINE_ID")

    if not all([OPENAI_API_KEY, SEARCH_ENGINE_ID]):
        raise ValueError("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã¨ SEARCH_ENGINE_ID ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

    # æ­¥é©Ÿ 3: åŸ·è¡Œä¸»è¦é‚è¼¯
    Google Search_service = get_Google Search_service()

    keywords_str = "éƒ½å¸‚è¨ˆç”»å›³,ç”¨é€”åœ°åŸŸ,å»ºè”½ç‡,å®¹ç©ç‡,é–‹ç™ºæŒ‡å°è¦ç¶±,å»ºç¯‰åŸºæº–æ³•"
    keywords = keywords_str.split(',')
    max_search_results = 10
    max_links_to_crawl = 20
    extractor_model = "o3"

    query = build_query(city, keywords)
    print(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {query}")

    seed_links = search_links(Google Search_service, query, SEARCH_ENGINE_ID, num_results=max_search_results)
    print(f"ğŸŒ± ã‚·ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã‚’{len(seed_links)}ä»¶è¦‹ã¤ã‘ã¾ã—ãŸã€‚")

    if not seed_links:
        return {"error": "ã‚·ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚"}

    first_link_domain = urlparse(seed_links[0]).netloc
    base_domain_str = tldextract.extract(first_link_domain).registered_domain
    print(f"ğŸ”— ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã®ãƒ™ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³: {base_domain_str}")
    
    all_links = bfs(seed_links, base_domain_str, max_depth=1, max_total=max_links_to_crawl)
    print(f"ğŸ”— åˆè¨ˆ{len(all_links)}ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¾ã—ãŸï¼ˆã‚·ãƒ¼ãƒ‰å«ã‚€ï¼‰ã€‚")

    relevant_links = []
    print("\né–¢é€£ãƒªãƒ³ã‚¯ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...")
    for link in all_links:
        if is_link_relevant(link, keywords, OPENAI_API_KEY):
            print(f"âœ… é–¢é€£ã‚ã‚Š: {link}")
            relevant_links.append(link)
        else:
            print(f"âŒ é–¢é€£ãªã—: {link}")
    
    print(f"\né–¢é€£ãƒªãƒ³ã‚¯ã‚’{len(relevant_links)}ä»¶è¦‹ã¤ã‘ã¾ã—ãŸã€‚")

    PDF_DIR = Path("downloaded_pdfs")
    PDF_DIR.mkdir(exist_ok=True)
    
    all_extracted_findings = []
    all_extracted_external_links = []
    
    with httpx.Client(timeout=30.0, follow_redirects=True) as client:
        for link in relevant_links:
            pdf_path = download_pdf_if_available(link, base_domain_str, client, PDF_DIR)
            path_to_process = pdf_path if pdf_path else link
            
            print(f"\nè¦ç´„ä¸­: {path_to_process}")
            summary = summarize_text_from_url_or_pdf(path_to_process, city, OPENAI_API_KEY, extractor_model)
            
            if summary and summary.get('findings'):
                for finding in summary['findings']:
                    finding_with_source = {'source': path_to_process, 'finding': finding}
                    all_extracted_findings.append(finding_with_source)
            if summary and summary.get('external_links'):
                for ext_link in summary['external_links']:
                    link_with_source = {'source': path_to_process, 'external_link': ext_link}
                    all_extracted_external_links.append(link_with_source)

    REPORTS_DIR = Path("generated_reports")
    REPORTS_DIR.mkdir(exist_ok=True)

    zone_report_path = REPORTS_DIR / "zone_regulations_report.txt"
    sources_report_path = REPORTS_DIR / "data_sources_report.txt"
    
    generate_zone_regulations_txt(all_extracted_findings, zone_report_path)
    generate_sources_txt(all_extracted_findings, all_extracted_external_links, sources_report_path)

    final_result = {
        "zone_report": zone_report_path.read_text(encoding='utf-8') if zone_report_path.exists() else "ç”¨é€”åœ°åŸŸãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
        "sources_report": sources_report_path.read_text(encoding='utf-8') if sources_report_path.exists() else "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    }
    return final_result
