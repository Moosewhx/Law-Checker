# backend_logic/main_runner.py
# --------------------------------
import os
from dotenv import load_dotenv
from pathlib import Path
import httpx
from urllib.parse import urlparse
import tldextract

from .initialize_credentials import initialize_google_credentials
from .search_google import (
    build_query,
    search_links,
    get_google_search_service,
)
from .ai_filter import is_link_relevant
from .link_crawler import bfs
from .pdf_downloader import download_pdf_if_available
from .summarizer import (
    summarize_text_from_url_or_pdf,
    generate_zone_regulations_txt,
    generate_sources_txt,
)


def run_analysis_for_city(city: str) -> dict:
    """éƒ½å¸‚è¨ˆç”»é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åé›†ãƒ»è¦ç´„ã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
    load_dotenv()

    # Google ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼
    try:
        initialize_google_credentials()
        print("âœ… Google ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®èªè¨¼ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
    except Exception as e:
        return {"error": f"Google èªè¨¼ã‚¨ãƒ©ãƒ¼: {e}"}

    openai_api_key = os.getenv("OPENAI_API_KEY")
    search_engine_id = os.getenv("SEARCH_ENGINE_ID")
    if not openai_api_key or not search_engine_id:
        raise ValueError(
            "ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã¨ SEARCH_ENGINE_ID ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        )

    google_search_service = get_google_search_service()

    keywords = [
        "éƒ½å¸‚è¨ˆç”»å›³",
        "ç”¨é€”åœ°åŸŸ",
        "å»ºè”½ç‡",
        "å®¹ç©ç‡",
        "é–‹ç™ºæŒ‡å°è¦ç¶±",
        "å»ºç¯‰åŸºæº–æ³•",
    ]
    max_search_results = 10
    max_links_to_crawl = 20
    extractor_model = "o3"

    # 1. æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆ
    query = build_query(city, keywords)
    print(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {query}")

    # 2. Google æ¤œç´¢ã§ã‚·ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯å–å¾—
    seed_links = search_links(
        google_search_service, query, search_engine_id, num_results=max_search_results
    )
    print(f"ğŸŒ± ã‚·ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã‚’ {len(seed_links)} ä»¶è¦‹ã¤ã‘ã¾ã—ãŸã€‚")
    if not seed_links:
        return {"error": "ã‚·ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚"}

    # 3. åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«
    first_link_domain = urlparse(seed_links[0]).netloc
    base_domain_str = tldextract.extract(first_link_domain).registered_domain
    print(f"ğŸ”— ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³: {base_domain_str}")

    all_links = bfs(seed_links, base_domain_str, max_depth=1, max_total=max_links_to_crawl)
    print(f"ğŸ”— ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒªãƒ³ã‚¯ç·æ•°ï¼ˆã‚·ãƒ¼ãƒ‰å«ã‚€ï¼‰: {len(all_links)}")

    # 4. AI ã§é–¢é€£ãƒªãƒ³ã‚¯ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    relevant_links = []
    print("\né–¢é€£ãƒªãƒ³ã‚¯ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...")
    for link in all_links:
        if is_link_relevant(link, keywords, openai_api_key):
            print(f"âœ… é–¢é€£ã‚ã‚Š: {link}")
            relevant_links.append(link)
        else:
            print(f"âŒ é–¢é€£ãªã—: {link}")
    print(f"\né–¢é€£ãƒªãƒ³ã‚¯ã‚’ {len(relevant_links)} ä»¶ç™ºè¦‹ã€‚")

    # 5. PDF ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ & è¦ç´„
    pdf_dir = Path("downloaded_pdfs")
    pdf_dir.mkdir(exist_ok=True)

    all_findings = []
    all_external_links = []

    with httpx.Client(timeout=30.0, follow_redirects=True) as client:
        for link in relevant_links:
            pdf_path = download_pdf_if_available(link, base_domain_str, client, pdf_dir)
            target = pdf_path if pdf_path else link

            print(f"\nè¦ç´„å‡¦ç†: {target}")
            summary = summarize_text_from_url_or_pdf(
                target, city, openai_api_key, extractor_model
            )

            if summary and summary.get("findings"):
                all_findings.extend(
                    {"source": target, "finding": f} for f in summary["findings"]
                )
            if summary and summary.get("external_links"):
                all_external_links.extend(
                    {"source": target, "external_link": l}
                    for l in summary["external_links"]
                )

    # 6. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    reports_dir = Path("generated_reports")
    reports_dir.mkdir(exist_ok=True)
    zone_report_path = reports_dir / "zone_regulations_report.txt"
    sources_report_path = reports_dir / "data_sources_report.txt"

    generate_zone_regulations_txt(all_findings, zone_report_path)
    generate_sources_txt(all_findings, all_external_links, sources_report_path)

    return {
        "zone_report": zone_report_path.read_text(encoding="utf-8")
        if zone_report_path.exists()
        else "ç”¨é€”åœ°åŸŸãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
        "sources_report": sources_report_path.read_text(encoding="utf-8")
        if sources_report_path.exists()
        else "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
    }
