"""
„É≠„Éº„Ç´„É´Áâà main.py „Å®Á≠â‰æ°„ÅÆÂá¶ÁêÜ„ÇíÂÆüË°å„Åó FastAPI „Åã„ÇâÂëº„Å≥Âá∫„Åõ„ÇãÈñ¢Êï∞„Å´„Åó„Åü„ÄÇ
"""

from __future__ import annotations
import os, json, time
from pathlib import Path
from urllib.parse import urlparse

import httpx, urllib3
from pypdf import PdfReader
from bs4 import BeautifulSoup
from dotenv import load_dotenv

from .search_google import build_query, search_links
from .ai_filter import is_link_relevant
from .link_crawler import bfs
from .pdf_downloader import download_pdf_if_available
from .summarizer import summarize_text_from_url_or_pdf

urllib3.disable_warnings()


def _pdf_text(pdf_path: Path) -> str:
    reader = PdfReader(str(pdf_path))
    return "\n".join(p.extract_text() or "" for p in reader.pages)


def _html_text(url: str) -> str:
    with httpx.Client(timeout=20.0, verify=False) as client:
        r = client.get(url)
        soup = BeautifulSoup(r.text, "html.parser")
        return soup.get_text(" ", strip=True)


def _write_reports(findings, ext_links, zone_path: Path, src_path: Path):
    zone_content = "Áî®ÈÄîÂú∞Âüü„É¨„Éù„Éº„Éà\n=========================\n\n"
    if findings:
        for f in findings:
            zone_content += f"- {f.get('regulation_type', '')}: {f.get('value', '')}\n"
            if f.get('zone'):
                zone_content += f"  Âú∞Âüü: {f.get('zone')}\n"
            if f.get('condition'):
                zone_content += f"  Êù°‰ª∂: {f.get('condition')}\n"
            zone_content += "\n"
    else:
        zone_content += "ÂÖ∑‰ΩìÁöÑ„Å™Ë¶èÂà∂„ÇÑ„Éá„Éº„Çø„Éù„Ç§„É≥„Éà„ÅØË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ\n"
    
    zone_path.write_text(zone_content, encoding="utf-8")
    
    src_content = "„Éá„Éº„Çø„ÇΩ„Éº„Çπ„É¨„Éù„Éº„Éà\n====================\n\n"
    src_content += "### Findings\n"
    for f in findings:
        src_content += f"„ÇΩ„Éº„Çπ: {f.get('source_document_key', '')}\n"
        src_content += f"Ë¶èÂà∂: {f.get('regulation_type', '')}\n\n"
    
    src_content += "\n### External Links\n"
    for l in ext_links:
        src_content += f"„É™„É≥„ÇØ: {l.get('url', '')}\n"
        src_content += f"Ë™¨Êòé: {l.get('text', '')}\n\n"
    
    src_path.write_text(src_content, encoding="utf-8")


def run_analysis_for_city(city: str) -> dict:
    """„É≠„Éº„Ç´„É´Áâà„Å®Âêå‰∏Ä„É≠„Ç∏„ÉÉ„ÇØ"""
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    SERPER_API_KEY = os.getenv("SERPER_API_KEY")
    
    if not OPENAI_API_KEY or not SERPER_API_KEY:
        raise RuntimeError("OPENAI_API_KEY „Å® SERPER_API_KEY „ÇíË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")

    keywords = [
        "ÈÉΩÂ∏ÇË®àÁîªÂõ≥",
        "Áî®ÈÄîÂú∞Âüü", 
        "Âª∫ËîΩÁéá",
        "ÂÆπÁ©çÁéá",
        "ÈñãÁô∫ÊåáÂ∞éË¶ÅÁ∂±",
        "Âª∫ÁØâÂü∫Ê∫ñÊ≥ï",
    ]
    
    query = build_query(city, keywords)
    print("üîç Initial Search Query:", query)

    seed_links = search_links(query, SERPER_API_KEY, 10)
    print("üå± Found", len(seed_links), "seed links.")
    
    if not seed_links:
        return {"error": "„Ç∑„Éº„Éâ„É™„É≥„ÇØ„ÅåÂèñÂæó„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ"}

    base_domain = urlparse(seed_links[0]).netloc
    all_links = bfs(seed_urls=seed_links, base_domain_str=base_domain, max_depth=2, max_total=30)
    print("üîó Crawled to", len(all_links), "total unique links (including seeds).")

    pdf_dir = Path("downloaded_pdfs")
    pdf_dir.mkdir(exist_ok=True)
    reports_dir = Path("generated_reports")
    reports_dir.mkdir(exist_ok=True)

    findings, ext_links, pdf_urls = [], [], []

    with httpx.Client(timeout=20.0, verify=False) as client:
        for idx, link in enumerate(all_links, 1):
            if idx > 30:
                break
                
            if not is_link_relevant(link, city, base_domain, OPENAI_API_KEY):
                print("‚ùå [Filter] Skipping:", link)
                continue
                
            print(f"‚úÖ [Extract] ({idx}/30):", link)

            pdf_path = download_pdf_if_available(link, pdf_dir, client)
            
            if pdf_path:
                body = _pdf_text(pdf_path)
                doc_key = str(pdf_path)
                pdf_urls.append(f"/files/{pdf_path.name}")
            else:
                body = _html_text(link)
                doc_key = link

            summary = summarize_text_from_url_or_pdf(
                doc_key, city, OPENAI_API_KEY, "o3"
            )
            
            if summary:
                try:
                    if isinstance(summary, dict):
                        findings.extend(summary.get("findings", []))
                        ext_links.extend(summary.get("external_links", []))
                    else:
                        obj = json.loads(summary)
                        findings.extend(obj.get("findings", []))
                        ext_links.extend(obj.get("external_links", []))
                except (json.JSONDecodeError, TypeError) as e:
                    print("‚ö†Ô∏è JSON parse error:", doc_key, e)

            time.sleep(1.0)

    zone_path = reports_dir / "zone_regulations_report.txt"
    src_path = reports_dir / "data_sources_report.txt"
    _write_reports(findings, ext_links, zone_path, src_path)

    return {
        "zone_report": zone_path.read_text(encoding="utf-8"),
        "sources_report": src_path.read_text(encoding="utf-8"),
        "pdf_download_urls": pdf_urls,
    }
