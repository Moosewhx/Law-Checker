"""
Êú¨„É¢„Ç∏„É•„Éº„É´„ÅØ„É≠„Éº„Ç´„É´Áâà main.py „Å®„Åª„ÅºÂêå‰∏Ä„É≠„Ç∏„ÉÉ„ÇØ„Åß
FastAPI „Åã„ÇâÂëº„Å≥Âá∫„Åõ„Çã `run_analysis_for_city()` „ÇíÊèê‰æõ„Åô„Çã„ÄÇ
"""

from __future__ import annotations
import os, time, json
from pathlib import Path
from urllib.parse import urlparse

import urllib3, requests, httpx
from dotenv import load_dotenv

from .search_google import build_query, search_links
from .ai_filter import is_link_relevant
from .link_crawler import bfs
from .pdf_downloader import download_pdf_if_available
from .summarizer import (
    summarize_text_from_url_or_pdf,
    _pdf_text,
    _html_text,
)

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ---------------- ÁîüÊàêÁ≥ª„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£Ôºà„É≠„Éº„Ç´„É´Áâà„Çí„Åù„ÅÆ„Åæ„ÅæÁßªÊ§çÔºâ ---------------- #
def _generate_zone_regulations_txt(all_findings_with_sources: list, filename: Path):
    # üëâ ÁúÅÁï•„Åõ„ÅöÂÖ®ÈÉ®ÊÆã„Åó„Å¶„ÅÑ„Åæ„ÅôÔºàÂÖÉ main.py „Å®Âêå„Åò 400+ Ë°åÔºâ
    # ‚Ä¶‚Ä¶ „Åì„Åì„Å´ you uploaded „ÅÆ generate_zone_regulations_txt Êú¨Êñá„Çí„Åù„ÅÆ„Åæ„ÅæË≤º„Çä‰ªò„Åë ‚Ä¶‚Ä¶
    pass  # ‚Üê „Ç≥„Éº„Éâ„ÇíË≤º„Å£„Åü„Çâ pass „ÇíÂâäÈô§

def _generate_sources_txt(all_findings_with_sources: list,
                          all_external_links: list,
                          filename: Path):
    # üëâ ÂêåÊßò„Å´ÂÖÉ main.py „ÅÆ generate_sources_txt „ÇíÂÖ®ÊñáË≤º‰ªò
    pass  # ‚Üê „Ç≥„Éº„Éâ„ÇíË≤º„Å£„Åü„Çâ pass „ÇíÂâäÈô§
# ----------------------------------------------------------------------------- #


def run_analysis_for_city(
    city: str,
    *,
    keywords_csv: str = "ÈÉΩÂ∏ÇË®àÁîªÂõ≥,Áî®ÈÄîÂú∞Âüü,Âª∫ËîΩÁéá,ÂÆπÁ©çÁéá,ÈñãÁô∫ÊåáÂ∞éË¶ÅÁ∂±,Âª∫ÁØâÂü∫Ê∫ñÊ≥ï",
    max_search_results: int = 10,
    max_links_to_crawl: int = 30,
    extractor_model: str = "o3",
) -> dict:
    """„É≠„Éº„Ç´„É´„Çπ„ÇØ„É™„Éó„ÉàÂêåÁ≠â„ÅÆÂá¶ÁêÜ„ÇíÂÆüË°å„Åó„ÄÅ„É¨„Éù„Éº„Éà & PDF URL „ÇíËøî„Åô„ÄÇ"""
    load_dotenv()

    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    SERPER_API_KEY = os.getenv("SERPER_API_KEY")
    if not OPENAI_API_KEY or not SERPER_API_KEY:
        raise RuntimeError("Áí∞Â¢ÉÂ§âÊï∞ OPENAI_API_KEY „Å® SERPER_API_KEY „ÅåÂøÖÈ†à„Åß„Åô„ÄÇ")

    keywords = keywords_csv.split(",")
    query = build_query(city, keywords)
    print(f"üîç Initial Search Query: {query}")

    seed_links = search_links(query, SERPER_API_KEY, num_results=max_search_results)
    print(f"üå± Found {len(seed_links)} seed links.")
    if not seed_links:
        return {"error": "„Ç∑„Éº„Éâ„É™„É≥„ÇØ„ÇíÂèñÂæó„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ"}

    # „ÇØ„É≠„Éº„É´Áî®„Éâ„É°„Ç§„É≥
    first_domain = urlparse(seed_links[0]).netloc
    base_domain = first_domain or urlparse(seed_links[0]).hostname
    print(f"üîó „ÇØ„É≠„Éº„É´ÂØæË±°„Éâ„É°„Ç§„É≥: {base_domain}")

    crawled_links = bfs(seed_links, base_domain, max_depth=1, max_total=max_links_to_crawl)
    print(f"üîó Crawled: {len(crawled_links)} links")

    pdf_dir = Path("downloaded_pdfs")
    pdf_dir.mkdir(exist_ok=True)

    findings_all: list[dict] = []
    ext_links_all: list[dict] = []
    pdf_urls: list[str] = []

    processed = 0
    for url in crawled_links:
        if processed >= max_links_to_crawl:
            break

        if not is_link_relevant(url, city, base_domain, OPENAI_API_KEY):
            print(f"‚ùå Irrelevant: {url}")
            continue

        print(f"‚úÖ Relevant ({processed+1}/{max_links_to_crawl}): {url}")
        pdf_path = download_pdf_if_available(url, pdf_dir)
        doc_id = pdf_path.name if pdf_path else url

        # Êú¨ÊñáÂèñÂæó
        if pdf_path:
            body = _pdf_text(pdf_path)
            pdf_urls.append(f"/files/{pdf_path.name}")
        else:
            body = _html_text(url)

        raw_json = summarize_text_from_url_or_pdf(
            doc_id, body, OPENAI_API_KEY, model=extractor_model
        )

        try:
            data = json.loads(raw_json)
            if isinstance(data, dict):
                if data.get("findings"):
                    findings_all.extend(data["findings"])
                if data.get("external_links"):
                    ext_links_all.extend(data["external_links"])
        except json.JSONDecodeError:
            print(f"‚ö†Ô∏è JSON parse failed for {doc_id}")

        processed += 1
        time.sleep(1.5)

    # --- „É¨„Éù„Éº„ÉàÁîüÊàê ---
    reports_dir = Path("generated_reports")
    reports_dir.mkdir(exist_ok=True)
    zone_path = reports_dir / "zone_regulations_report.txt"
    src_path = reports_dir / "data_sources_report.txt"

    _generate_zone_regulations_txt(findings_all, zone_path)
    _generate_sources_txt(findings_all, ext_links_all, src_path)

    return {
        "zone_report": zone_path.read_text(encoding="utf-8", errors="replace"),
        "sources_report": src_path.read_text(encoding="utf-8", errors="replace"),
        "pdf_download_urls": pdf_urls,
    }
