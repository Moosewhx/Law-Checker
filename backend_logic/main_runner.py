‚Äú‚Äù‚Äù
„É≠„Éº„Ç´„É´Áâà main.py „Å®Á≠â‰æ°„ÅÆÂá¶ÁêÜ„ÇíÂÆüË°å„Åó FastAPI „Åã„ÇâÂëº„Å≥Âá∫„Åõ„ÇãÈñ¢Êï∞„Å´„Åó„Åü„ÄÇ
‚Äú‚Äù‚Äù

from **future** import annotations
import os, json, time
from pathlib import Path
from urllib.parse import urlparse

import httpx, urllib3
from pypdf import PdfReader
from bs4 import BeautifulSoup
from dotenv import load_dotenv

from .search_google import build_query, search_links
from .ai_filter import is_link_relevant
from .link_crawler import bfs
from .pdf_downloader import download_pdf_if_available
from .summarizer import summarize_text_from_url_or_pdf

urllib3.disable_warnings()

def _pdf_text(pdf_path: Path) -> str:
reader = PdfReader(str(pdf_path))
return ‚Äú\n‚Äù.join(p.extract_text() or ‚Äú‚Äù for p in reader.pages)

def _html_text(url: str) -> str:
with httpx.Client(timeout=20.0, verify=False) as client:
r = client.get(url)
soup = BeautifulSoup(r.text, ‚Äúhtml.parser‚Äù)
return soup.get_text(‚Äù ‚Äú, strip=True)

def _write_reports(findings, ext_links, zone_path: Path, src_path: Path):
zone_content = ‚ÄúÁî®ÈÄîÂú∞Âüü„É¨„Éù„Éº„Éà\n=========================\n\n‚Äù
if findings:
for f in findings:
zone_content += f‚Äù- {f.get(‚Äòregulation_type‚Äô, ‚Äò‚Äô)}: {f.get(‚Äòvalue‚Äô, ‚Äò‚Äô)}\n‚Äù
if f.get(‚Äòzone‚Äô):
zone_content += f‚Äù  Âú∞Âüü: {f.get(‚Äòzone‚Äô)}\n‚Äù
if f.get(‚Äòcondition‚Äô):
zone_content += f‚Äù  Êù°‰ª∂: {f.get(‚Äòcondition‚Äô)}\n‚Äù
zone_content += ‚Äú\n‚Äù
else:
zone_content += ‚ÄúÂÖ∑‰ΩìÁöÑ„Å™Ë¶èÂà∂„ÇÑ„Éá„Éº„Çø„Éù„Ç§„É≥„Éà„ÅØË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ\n‚Äù

```
zone_path.write_text(zone_content, encoding="utf-8")

src_content = "„Éá„Éº„Çø„ÇΩ„Éº„Çπ„É¨„Éù„Éº„Éà\n====================\n\n"
src_content += "### Findings\n"
for f in findings:
    src_content += f"„ÇΩ„Éº„Çπ: {f.get('source_document_key', '')}\n"
    src_content += f"Ë¶èÂà∂: {f.get('regulation_type', '')}\n\n"

src_content += "\n### External Links\n"
for l in ext_links:
    src_content += f"„É™„É≥„ÇØ: {l.get('url', '')}\n"
    src_content += f"Ë™¨Êòé: {l.get('text', '')}\n\n"

src_path.write_text(src_content, encoding="utf-8")
```

def run_analysis_for_city(city: str) -> dict:
‚Äú‚Äù‚Äú„É≠„Éº„Ç´„É´Áâà„Å®Âêå‰∏Ä„É≠„Ç∏„ÉÉ„ÇØ‚Äù‚Äù‚Äù
load_dotenv()
OPENAI_API_KEY = os.getenv(‚ÄúOPENAI_API_KEY‚Äù)
SERPER_API_KEY = os.getenv(‚ÄúSERPER_API_KEY‚Äù)

```
if not OPENAI_API_KEY or not SERPER_API_KEY:
    raise RuntimeError("OPENAI_API_KEY „Å® SERPER_API_KEY „ÇíË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")

keywords = [
    "ÈÉΩÂ∏ÇË®àÁîªÂõ≥",
    "Áî®ÈÄîÂú∞Âüü", 
    "Âª∫ËîΩÁéá",
    "ÂÆπÁ©çÁéá",
    "ÈñãÁô∫ÊåáÂ∞éË¶ÅÁ∂±",
    "Âª∫ÁØâÂü∫Ê∫ñÊ≥ï",
]

query = build_query(city, keywords)
print("üîç Initial Search Query:", query)

seed_links = search_links(query, SERPER_API_KEY, 10)
print("üå± Found", len(seed_links), "seed links.")

if not seed_links:
    return {"error": "„Ç∑„Éº„Éâ„É™„É≥„ÇØ„ÅåÂèñÂæó„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ"}

base_domain = urlparse(seed_links[0]).netloc
all_links = bfs(seed_urls=seed_links, base_domain_str=base_domain, max_depth=2, max_total=30)
print("üîó Crawled to", len(all_links), "total unique links (including seeds).")

pdf_dir = Path("downloaded_pdfs")
pdf_dir.mkdir(exist_ok=True)
reports_dir = Path("generated_reports")
reports_dir.mkdir(exist_ok=True)

findings, ext_links, pdf_urls = [], [], []

with httpx.Client(timeout=20.0, verify=False) as client:
    for idx, link in enumerate(all_links, 1):
        if idx > 30:
            break
            
        if not is_link_relevant(link, city, base_domain, OPENAI_API_KEY):
            print("‚ùå [Filter] Skipping:", link)
            continue
            
        print(f"‚úÖ [Extract] ({idx}/30):", link)

        pdf_path = download_pdf_if_available(link, pdf_dir, client)
        
        if pdf_path:
            body = _pdf_text(pdf_path)
            doc_key = str(pdf_path)
            pdf_urls.append(f"/files/{pdf_path.name}")
        else:
            body = _html_text(link)
            doc_key = link

        summary = summarize_text_from_url_or_pdf(
            doc_key, city, OPENAI_API_KEY, "o3"
        )
        
        if summary:
            try:
                if isinstance(summary, dict):
                    findings.extend(summary.get("findings", []))
                    ext_links.extend(summary.get("external_links", []))
                else:
                    obj = json.loads(summary)
                    findings.extend(obj.get("findings", []))
                    ext_links.extend(obj.get("external_links", []))
            except (json.JSONDecodeError, TypeError) as e:
                print("‚ö†Ô∏è JSON parse error:", doc_key, e)

        time.sleep(1.0)

zone_path = reports_dir / "zone_regulations_report.txt"
src_path = reports_dir / "data_sources_report.txt"
_write_reports(findings, ext_links, zone_path, src_path)

return {
    "zone_report": zone_path.read_text(encoding="utf-8"),
    "sources_report": src_path.read_text(encoding="utf-8"),
    "pdf_download_urls": pdf_urls,
}
```
