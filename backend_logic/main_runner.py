"""
FastAPI ç”¨: å¸‚åŒºç”ºæ‘ã‚’å—ã‘å–ã‚Šã€éƒ½å¸‚è¨ˆç”»é–¢é€£ PDF/HTML ã‚’åé›†ãƒ»è¦ç´„ã—ã¦
ãƒ¬ãƒãƒ¼ãƒˆã¨ PDF ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ URL ã‚’è¿”ã™ã€‚
ãƒ­ãƒ¼ã‚«ãƒ«ç‰ˆ main.py ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¸è¥²ã—ã€å¤–éƒ¨é–¢æ•° _pdf_text/_html_text ã¸ã¯ä¾å­˜ã—ãªã„ã€‚
"""
from __future__ import annotations
import os, time, json
from pathlib import Path
from urllib.parse import urlparse

import httpx
from bs4 import BeautifulSoup
from pypdf import PdfReader
from dotenv import load_dotenv

from .search_google import build_query, search_links
from .ai_filter import is_link_relevant
from .link_crawler import bfs
from .pdf_downloader import download_pdf_if_available
from .summarizer import summarize_text_from_url_or_pdf


# ===================== ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ===================== #
def _pdf_text(pdf_path: Path) -> str:
    """PDF ã‹ã‚‰å…¨ãƒšãƒ¼ã‚¸æ–‡å­—åˆ—ã‚’å–å¾—ï¼ˆéå¸¸ã«å˜ç´”ãªå®Ÿè£…ï¼‰ã€‚"""
    try:
        reader = PdfReader(str(pdf_path))
        return "\n".join(p.extract_text() or "" for p in reader.pages)
    except Exception as e:
        print(f"PDF èª­ã¿å–ã‚Šå¤±æ•—: {e}")
        return ""


def _html_text(url: str) -> str:
    """HTML ã‹ã‚‰ <body> ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠœç²‹ã€‚"""
    try:
        r = httpx.get(url, timeout=20.0, verify=False)
        soup = BeautifulSoup(r.text, "html.parser")
        return soup.get_text(" ", strip=True)
    except Exception as e:
        print(f"HTML å–å¾—å¤±æ•—: {e}")
        return ""
# ==================================================================== #


def _generate_zone_regulations_txt(findings: list[dict], path: Path):
    """ç”¨é€”åœ°åŸŸãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰ã€‚"""
    with path.open("w", encoding="utf-8") as f:
        for item in findings:
            f.write(f"- {item.get('finding', '')}\n")


def _generate_sources_txt(findings: list[dict], ext_links: list[dict], path: Path):
    """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰ã€‚"""
    with path.open("w", encoding="utf-8") as f:
        f.write("### Findings\n")
        for item in findings:
            f.write(f"{item.get('source_document_key','')}\n")
        f.write("\n### External Links\n")
        for item in ext_links:
            f.write(f"{item.get('external_link','')}\n")


# ===================== ãƒ¡ã‚¤ãƒ³é–¢æ•° ===================== #
def run_analysis_for_city(
    city: str,
    *,
    keywords_csv: str = (
        "éƒ½å¸‚è¨ˆç”»å›³,ç”¨é€”åœ°åŸŸ,å»ºè”½ç‡,å®¹ç©ç‡,é–‹ç™ºæŒ‡å°è¦ç¶±,å»ºç¯‰åŸºæº–æ³•"
    ),
    max_search_results: int = 10,
    max_links_to_crawl: int = 20,
    extractor_model: str = "o3",
) -> dict:
    load_dotenv()

    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    SERPER_API_KEY = os.getenv("SERPER_API_KEY")
    if not OPENAI_API_KEY or not SERPER_API_KEY:
        raise RuntimeError("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã¨ SERPER_API_KEY ãŒå¿…è¦ã§ã™ã€‚")

    keywords = keywords_csv.split(",")
    query = build_query(city, keywords)
    print(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}")

    seed_links = search_links(query, SERPER_API_KEY, num_results=max_search_results)
    print(f"ğŸŒ± ã‚·ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯: {len(seed_links)} ä»¶")
    if not seed_links:
        return {"error": "ã‚·ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"}

    # ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³
    first_domain = urlparse(seed_links[0]).netloc
    base_domain = first_domain or urlparse(seed_links[0]).hostname
    print(f"ğŸ”— ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡: {base_domain}")

    all_links = bfs(seed_links, base_domain, max_depth=1, max_total=max_links_to_crawl)
    print(f"ğŸ”— ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†: {len(all_links)} ä»¶")

    pdf_dir = Path("downloaded_pdfs")
    pdf_dir.mkdir(exist_ok=True)

    findings: list[dict] = []
    ext_links: list[dict] = []
    pdf_urls: list[str] = []

    for url in all_links:
        if not is_link_relevant(url, city, base_domain, OPENAI_API_KEY):
            print(f"âŒ ç„¡é–¢ä¿‚: {url}")
            continue
        print(f"âœ… é–¢é€£: {url}")

        pdf_path = download_pdf_if_available(url, pdf_dir)
        doc_key = str(pdf_path) if pdf_path else url

        # æœ¬æ–‡æŠ½å‡º
        content = _pdf_text(pdf_path) if pdf_path else _html_text(url)
        if pdf_path:
            pdf_urls.append(f"/files/{pdf_path.name}")

        # GPT è¦ç´„
        summary = summarize_text_from_url_or_pdf(
            doc_key, content, OPENAI_API_KEY, extractor_model
        )

        try:
            data = json.loads(summary) if summary else {}
            findings.extend(data.get("findings", []))
            ext_links.extend(data.get("external_links", []))
        except json.JSONDecodeError:
            print(f"âš ï¸ JSON è§£æå¤±æ•—: {doc_key}")

        if len(findings) >= max_links_to_crawl:
            break

        time.sleep(1.2)  # API ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    reports_dir = Path("generated_reports")
    reports_dir.mkdir(exist_ok=True)
    zone_path = reports_dir / "zone_regulations_report.txt"
    src_path = reports_dir / "data_sources_report.txt"

    _generate_zone_regulations_txt(findings, zone_path)
    _generate_sources_txt(findings, ext_links, src_path)

    return {
        "zone_report": zone_path.read_text(encoding="utf-8", errors="replace"),
        "sources_report": src_path.read_text(encoding="utf-8", errors="replace"),
        "pdf_download_urls": pdf_urls,
    }
