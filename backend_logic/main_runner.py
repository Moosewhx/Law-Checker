"""
ä¸æœ¬åœ°ç‰ˆæœ¬å®Œå…¨ä¸€è‡´çš„é“¾æ¥æŸ¥æ‰¾å’Œè¿‡æ»¤é€»è¾‘ï¼Œä¸“æ³¨äºPDFä¸‹è½½å’Œé“¾æ¥è¾“å‡º
"""

from __future__ import annotations
import os, time
from pathlib import Path
from urllib.parse import urlparse
import urllib3
from dotenv import load_dotenv

from .search_google import build_query, search_links
from .ai_filter import is_link_relevant
from .link_crawler import bfs
from .pdf_downloader import download_pdf_if_available

urllib3.disable_warnings()


def run_analysis_for_city(city: str) -> dict:
    """ä¸æœ¬åœ°ç‰ˆæœ¬å®Œå…¨ä¸€è‡´çš„é“¾æ¥æŸ¥æ‰¾å’Œè¿‡æ»¤"""
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    SERPER_API_KEY = os.getenv("SERPER_API_KEY")
    
    if not OPENAI_API_KEY or not SERPER_API_KEY:
        raise RuntimeError("OPENAI_API_KEY ã¨ SERPER_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

    # ä¸æœ¬åœ°ç‰ˆæœ¬å®Œå…¨ä¸€è‡´çš„å…³é”®è¯
    keywords = [
        "éƒ½å¸‚è¨ˆç”»å›³",
        "ç”¨é€”åœ°åŸŸ", 
        "å»ºè”½ç‡",
        "å®¹ç©ç‡",
        "é–‹ç™ºæŒ‡å°è¦ç¶±",
        "å»ºç¯‰åŸºæº–æ³•"
    ]
    
    query = build_query(city, keywords)
    print(f"ğŸ” Initial Search Query: {query}")

    # ä¸æœ¬åœ°ç‰ˆæœ¬ä¸€è‡´çš„æœç´¢å‚æ•°
    seed_links = search_links(query, SERPER_API_KEY, num_results=10)
    print(f"ğŸŒ± Found {len(seed_links)} seed links.")
    
    if not seed_links:
        return {"error": "ã‚·ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"}

    # ä¸æœ¬åœ°ç‰ˆæœ¬å®Œå…¨ä¸€è‡´çš„çˆ¬è™«å‚æ•°
    crawled_links = bfs(seed_links, seed_links[0], max_depth=2, max_total=120)
    print(f"ğŸ”— Crawled to {len(crawled_links)} total unique links (including seeds).")

    pdf_dir = Path("downloaded_pdfs")
    pdf_dir.mkdir(exist_ok=True)

    relevant_links = []
    pdf_downloads = []
    processed_count = 0
    
    # ä¸æœ¬åœ°ç‰ˆæœ¬ä¸€è‡´çš„å¤„ç†é€»è¾‘
    for url in crawled_links:
        if processed_count >= 30:  # ä¸æœ¬åœ°ç‰ˆæœ¬ä¸€è‡´çš„max_linksé™åˆ¶
            print(f"Reached max_links limit of 30. Stopping processing.")
            break

        # æå–base_domainç”¨äºè¿‡æ»¤
        base_domain = urlparse(seed_links[0]).netloc if seed_links else ""
        
        if not is_link_relevant(url, city, base_domain, OPENAI_API_KEY):
            print(f"âŒ [Filter] Skipping irrelevant link: {url}")
            continue
        
        print(f"âœ… [Relevant] Processing link ({processed_count + 1}/30): {url}")
        
        link_info = {
            "url": url,
            "type": "PDF" if url.lower().endswith(".pdf") else "HTML",
            "status": "relevant"
        }
        
        # å°è¯•ä¸‹è½½PDF
        if url.lower().endswith(".pdf"):
            pdf_path = download_pdf_if_available(url, str(pdf_dir))
            if pdf_path:
                link_info["downloaded"] = True
                link_info["local_path"] = f"/files/{Path(pdf_path).name}"
                pdf_downloads.append({
                    "original_url": url,
                    "local_path": f"/files/{Path(pdf_path).name}",
                    "filename": Path(pdf_path).name
                })
            else:
                link_info["downloaded"] = False
        else:
            link_info["downloaded"] = False
        
        relevant_links.append(link_info)
        processed_count += 1
        
        # ä¸æœ¬åœ°ç‰ˆæœ¬ä¸€è‡´çš„å»¶è¿Ÿ
        time.sleep(1.5)

    # ç”Ÿæˆç®€åŒ–æŠ¥å‘Š
    report_content = f"# {city} å»ºç¯‰è¦åˆ¶é–¢é€£ãƒªãƒ³ã‚¯èª¿æŸ»çµæœ\n\n"
    report_content += f"## æ¦‚è¦\n"
    report_content += f"- æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}\n"
    report_content += f"- åˆæœŸæ¤œç´¢çµæœ: {len(seed_links)} ä»¶\n"
    report_content += f"- ã‚¯ãƒ­ãƒ¼ãƒ«ç·æ•°: {len(crawled_links)} ä»¶\n"
    report_content += f"- é–¢é€£æ€§ã®é«˜ã„ãƒªãƒ³ã‚¯: {len(relevant_links)} ä»¶\n"
    report_content += f"- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸPDF: {len(pdf_downloads)} ä»¶\n\n"
    
    report_content += "## é–¢é€£æ€§ã®é«˜ã„ãƒªãƒ³ã‚¯ä¸€è¦§\n\n"
    for i, link in enumerate(relevant_links, 1):
        report_content += f"### {i}. {link['type']} ãƒªãƒ³ã‚¯\n"
        report_content += f"- URL: {link['url']}\n"
        if link['downloaded']:
            report_content += f"- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: æˆåŠŸ ({link['local_path']})\n"
        else:
            report_content += f"- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {'PDFä»¥å¤–' if not link['url'].lower().endswith('.pdf') else 'å¤±æ•—'}\n"
        report_content += "\n"
    
    if pdf_downloads:
        report_content += "## ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿PDFãƒ•ã‚¡ã‚¤ãƒ«\n\n"
        for pdf in pdf_downloads:
            report_content += f"- [{pdf['filename']}]({pdf['local_path']}) (å…ƒURL: {pdf['original_url']})\n"

    return {
        "summary": f"æ¤œç´¢å®Œäº†: {len(relevant_links)}ä»¶ã®é–¢é€£ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹ã€{len(pdf_downloads)}ä»¶ã®PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        "report": report_content,
        "relevant_links": relevant_links,
        "pdf_downloads": pdf_downloads,
        "statistics": {
            "total_crawled": len(crawled_links),
            "relevant_count": len(relevant_links),
            "pdf_count": len(pdf_downloads)
        }
    }
