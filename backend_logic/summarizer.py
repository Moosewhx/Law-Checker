import os
import openai
import json
from pypdf import PdfReader
import httpx
from bs4 import BeautifulSoup
from pathlib import Path
from google.cloud import vision

def get_text_from_pdf_with_vision(pdf_path):
    print(f"ğŸ‘ï¸ Vision API ã‚’ä½¿ç”¨ã—ã¦PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºä¸­: {pdf_path}")
    try:
        client = vision.ImageAnnotatorClient()
        with open(pdf_path, "rb") as image_file:
            content = image_file.read()
        image = vision.Image(content=content)
        response = client.document_text_detection(image=image)
        if response.error.message:
            raise Exception(f"{response.error.message}")
        return response.full_text_annotation.text
    except Exception as e:
        print(f"Vision API ã§ã®ã‚¨ãƒ©ãƒ¼ ({pdf_path}): {e}")
        print("-> PyPDF ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
        return get_text_from_pdf_with_pypdf(pdf_path)

def get_text_from_pdf_with_pypdf(pdf_path):
    try:
        with open(pdf_path, 'rb') as f:
            reader = PdfReader(f)
            text = "".join(page.extract_text() or "" for page in reader.pages)
        return text
    except Exception as e:
        print(f"PyPDF ã§ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({pdf_path}): {e}")
        return ""

def get_text_from_url(url, client):
    try:
        response = client.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        for script_or_style in soup(["script", "style", "nav", "footer", "header"]):
            script_or_style.decompose()
        text = soup.get_text(separator='\n', strip=True)
        return text
    except (httpx.RequestError, httpx.HTTPStatusError) as e:
        print(f"URLã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({url}): {e}")
        return ""

def summarize_text_from_url_or_pdf(path, city, api_key, model_choice):
    openai.api_key = api_key
    is_pdf = str(path).lower().endswith('.pdf')
    
    if is_pdf:
        text_content = get_text_from_pdf_with_vision(path)
    else:
        with httpx.Client(timeout=20.0, follow_redirects=True) as client:
            text_content = get_text_from_url(path, client)

    if not text_content or len(text_content.strip()) < 100:
        print("å†…å®¹ãŒçŸ­ã™ãã‚‹ã‹ç©ºã®ãŸã‚ã€è¦ç´„ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None

    model_map = {"o3": "gpt-3.5-turbo-0125", "o4": "gpt-4-turbo-2024-04-09"}
    model = model_map.get(model_choice, "gpt-3.5-turbo-0125")
    prompt = f"""
    You are a specialized data extractor for Japanese real estate and city planning regulations.
    Analyze the following text from the source related to the city of {city}.
    Your task is to extract very specific, quantifiable regulations and data points.
    Extract the following information if available:
    1.  **Zoning-related values (ç”¨é€”åœ°åŸŸé–¢é€£ã®æ•°å€¤):** Building coverage ratio (å»ºè”½ç‡), Floor area ratio (å®¹ç©ç‡), Height restrictions (é«˜ã•åˆ¶é™), Setback distances (å£é¢å¾Œé€€), Minimum lot size (æœ€ä½æ•·åœ°é¢ç©)
    2.  **Development guideline values (é–‹ç™ºæŒ‡å°è¦ç¶±é–¢é€£ã®æ•°å€¤):** Required park area percentage (å…¬åœ’ã®è¨­ç½®åŸºæº–), Parking space requirements (é§è»Šå ´ã®è¨­ç½®åŸºæº–), Road width requirements (é“è·¯ã®å¹…å“¡), Green space ratio (ç·‘åŒ–ç‡)
    3.  **Other specific regulations or requirements (ãã®ä»–ã®å…·ä½“çš„ãªè¦åˆ¶ã‚„è¦ä»¶):** Any other numerical requirements for development.
    4.  **External Links (å¤–éƒ¨ãƒªãƒ³ã‚¯):** Identify any hyperlinks mentioned in the text that point to external government sites, law databases, or related official documents.
    Present your output in a structured JSON format. The JSON should have two keys: "findings" and "external_links".
    Example format:
    {{
      "findings": [ "å»ºè”½ç‡ã¯60%ã§ã‚ã‚‹", "å®¹ç©ç‡ã¯200%ã§ã‚ã‚‹" ],
      "external_links": [ "https://elaws.e-gov.go.jp/document?lawid=341AC0000000201" ]
    }}
    If no specific information can be found, return an empty list for the corresponding key.
    Text to analyze:
    ---
    {text_content[:15000]}
    ---
    """
    try:
        response = openai.chat.completions.create(model=model, messages=[{"role": "system", "content": "You are a specialized data extractor for Japanese real estate and city planning regulations."}, {"role": "user", "content": prompt}], response_format={"type": "json_object"}, temperature=0.0)
        summary = response.choices[0].message.content
        return json.loads(summary)
    except Exception as e:
        print(f"OpenAI APIå‘¼ã³å‡ºã—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def generate_zone_regulations_txt(all_findings, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("ç”¨é€”åœ°åŸŸãƒ¬ãƒãƒ¼ãƒˆ\n=========================\n\n")
        if not all_findings:
            f.write("å…·ä½“çš„ãªè¦åˆ¶ã‚„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n")
            return
        findings_by_source = {}
        for item in all_findings:
            source = item['source']
            if source not in findings_by_source:
                findings_by_source[source] = []
            findings_by_source[source].append(item['finding'])
        for source, findings in findings_by_source.items():
            f.write(f"ã‚½ãƒ¼ã‚¹: {source}\n")
            for finding in findings:
                f.write(f"- {finding}\n")
            f.write("\n")

def generate_sources_txt(all_findings, all_external_links, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ\n====================\n\n")
        f.write("åˆ†æå¯¾è±¡ã®ãƒ—ãƒ©ã‚¤ãƒãƒªã‚½ãƒ¼ã‚¹:\n")
        primary_sources = sorted(list(set(item['source'] for item in all_findings)))
        if not primary_sources:
            f.write("åˆ†æã«æˆåŠŸã—ãŸãƒ—ãƒ©ã‚¤ãƒãƒªã‚½ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n")
        else:
            for source in primary_sources:
                f.write(f"- {source}\n")
        f.write("\n\n")
        f.write("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã§ç™ºè¦‹ã•ã‚ŒãŸå¤–éƒ¨ãƒªãƒ³ã‚¯:\n")
        ext_links = sorted(list(set(item['external_link'] for item in all_external_links)))
        if not ext_links:
            f.write("åˆ†æå¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å¤–éƒ¨ãƒªãƒ³ã‚¯ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n")
        else:
            for link in ext_links:
                f.write(f"- {link}\n")
